Platforms:
  marenostrum4:
    TYPE: slurm
    HOST: mn4-cluster1
        # This needs that a ~/.ssh/config exists with the following:
        # the login node in mn0 allows internet acces
        # Host mn4-cluster1
        #     Hostname mn0.bsc.es
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <bsc user>
    PROJECT: dese28
    HPC_PROJECT_DIR: /gpfs/projects/dese28
    USER: <to-be-overloaded-in-user-conf>
    QUEUE: debug
    SCRATCH_DIR: /gpfs/scratch
    FDB_DIR: /gpfs/projects/dese28/experiments
    ADD_PROJECT_TO_HOST: False
    TEMP_DIR: ''
    PROCESSORS_PER_NODE: 48
    CUSTOM_DIRECTIVES: "['#SBATCH --exclusive', '#SBATCH --export=ALL', '#SBATCH --hint=nomultithread']"
  marenostrum4-login:
    TYPE: ps
    HOST: mn4-cluster1
        # This needs that a ~/.ssh/config exists with the following:
        # the login node in mn0 allows internet acces
        # Host mn4-cluster1
        #     Hostname mn0.bsc.es
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <bsc user>
    PROJECT: dese28
    HPC_PROJECT_DIR: /gpfs/projects/dese28
    FDB_DIR: /gpfs/projects/dese28/experiments
    USER: <to-be-overloaded-in-user-conf>
    SCRATCH_DIR: /gpfs/scratch
    ADD_PROJECT_TO_HOST: False
    PARTITION: interactive
    PROCESSORS: 4
    MAX_WALLCLOCK: '02:00'
  juwels-login:
    TYPE: ps
    HOST: juwels-cluster
        # This needs that a ~/.ssh/config exists with the following:
        # Host juwels-cluster
        #     Hostname %.fz-juelich.de
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <juwels user>
    PROJECT: hhb19
    USER: <to-be-overloaded-in-user-conf>
    SCRATCH_DIR: /p/scratch
    SCRATCH_PROJECT_DIR: chhb19
    ADD_PROJECT_TO_HOST: False
    TEMP_DIR: ''
  juwels:
    TYPE: slurm
    HOST: juwels-cluster
        # This needs that a ~/.ssh/config exists with the following:
        # Host juwels-cluster
        #     Hostname %.fz-juelich.de
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <juwels user>
    PROJECT: hhb19
    USER: <to-be-overloaded-in-user-conf>
    QUEUE: ""
    CUSTOM_DIRECTIVES: '#SBATCH -p standard -N 1 --ntasks-per-node=24 --cpus-per-task=2'
    SCRATCH_DIR: /p/scratch
    SCRATCH_PROJECT_DIR: chhb19
    PARTITION: devel
    ADD_PROJECT_TO_HOST: False
    MAX_WALLCLOCK: '48:00'
    TEMP_DIR: ''
    PROCESSORS_PER_NODE: 24
  lumi-login:
    TYPE: ps
    HOST: lumi-cluster
        # This needs that a ~/.ssh/config exists with the following:
        # Host lumi-cluster
        #     Hostname lumi.csc.fi
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <lumi user>
    PROJECT: project_465000454
    HPC_PROJECT_DIR: /projappl/project_465000454
    FDB_DIR: /scratch/project_465000454/experiments
    FDB_PROD: "/users/lrb_465000454_fdb"
    USER: <to-be-overloaded-in-user-conf>
    SCRATCH_DIR: /scratch
    ADD_PROJECT_TO_HOST: False
    TEMP_DIR: ''
    DATABRIDGE_FDB_HOME: "/users/lrb_465000454_fdb/databridge"
    TOTAL_JOBS: 1
    MAX_WAITING_JOBS: 1
  lumi:
    TYPE: slurm
    HOST: lumi-cluster #lumi.csc.fi
        # This needs that a ~/.ssh/config exists with the following:
        # Host lumi-cluster
        #     Hostname lumi.csc.fi
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <lumi user>
    PROJECT: project_465000454
    HPC_PROJECT_DIR: /projappl/project_465000454
    FDB_DIR: /scratch/project_465000454/experiments
    USER: <to-be-overloaded-in-user-conf>
    SCRATCH_DIR: /scratch
    ADD_PROJECT_TO_HOST: False 
    TEMP_DIR: '' 
    BUDGET: "project_465000454"
    EXECUTABLE: "/bin/bash --login"
    DATABRIDGE_FDB_HOME: "/users/lrb_465000454_fdb/databridge"
    CUSTOM_DIRECTIVES: "['#SBATCH --exclusive', '#SBATCH --export=ALL', '#SBATCH --hint=nomultithread', '#SBATCH --mem=0']"
    APP_PARTITION: small
    FDB_PROD: "/users/lrb_465000454_fdb"
    MAX_PROCESSORS: 99999
    MAX_WALLCLOCK: "00:30"
  levante-login:
    TYPE: ps
    HOST: levante
        # This needs that a ~/.ssh/config exists with the following:
        # Host levante-cluster
        #     Hostname levante.dkrz.de
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <levante user>
    PROJECT: <to-be-overloaded-by-user>
    USER: <to-be-overloaded-in-user-conf>
    SCRATCH_DIR: /work
    ADD_PROJECT_TO_HOST: False
    TEMP_DIR: ''
    EXECUTABLE: "/usr/bin/bash -l" # this is needed on levante to make it find the module command
  levante:
    TYPE: slurm
    HOST: levante
        # This needs that a ~/.ssh/config exists with the following:
        # Host levante-cluster
        #     Hostname levante.dkrz.de
        #     IdentifyFile ~/.ssh/<your_sshkey>
        #     User <levante user>
    PROJECT: <to-be-overloaded-by-user>
    PARTITION: 'compute'
    USER: <to-be-overloaded-in-user-conf>
    QUEUE: ""
    SCRATCH_DIR: /work
    ADD_PROJECT_TO_HOST: False
    MAX_WALLCLOCK: '00:29'
    TEMP_DIR: ''
    PROCESSORS_PER_NODE: 128
    BUDGET: <to-be-overloaded-by-user>
    EXECUTABLE: "/usr/bin/bash -l" # this is needed on levante to make it find the module command

